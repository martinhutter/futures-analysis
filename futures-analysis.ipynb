{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3361e65a-61a2-43c2-8e0f-7e5bb5c3d1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing futures analysis with parameters:\n",
      "    Minimum Volume: 0\n",
      "    Minimum Days to Expiry: 0\n",
      "    Date Range: 1985-2026\n",
      "    Maximum Months Forward: 13\n",
      "    Trading Days Per Year: 251\n",
      "    Processing Commodities: CLA, COA, XBA, HOA, NGA, FNA, HG, ALE, GC, SI\n",
      "    \n",
      "\n",
      "Processing CLA...\n",
      "\n",
      "Starting processing for CLA\n",
      "Generated 504 tickers for CLA\n",
      "\n",
      "Processing metadata for CLA...\n",
      "Processing metadata batch 1/3 for CLA\n",
      "Processing metadata batch 2/3 for CLA\n",
      "Processing metadata batch 3/3 for CLA\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for CLA...\n",
      "Processing batch 1/11 for CLA\n",
      "Processing batch 2/11 for CLA\n",
      "Processing batch 3/11 for CLA\n",
      "Processing batch 4/11 for CLA\n",
      "Processing batch 5/11 for CLA\n",
      "Processing batch 6/11 for CLA\n",
      "Processing batch 7/11 for CLA\n",
      "Processing batch 8/11 for CLA\n",
      "Processing batch 9/11 for CLA\n",
      "Processing batch 10/11 for CLA\n",
      "Processing batch 11/11 for CLA\n",
      "Processing combined data with 2 columns\n",
      "Processing raw data with 2 columns\n",
      "Found 1 price columns and 1 volume columns\n",
      "Processed 1 active futures\n",
      "Successfully processed 1 active futures\n",
      "Determined last trade dates for 1 contracts\n",
      "\n",
      "Fetching interest rate data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for CLA...\n",
      "Determined last trade dates for 1 contracts\n",
      "Processing 1 dates...\n",
      "Completed futures data organization. Processed 0 dates with valid data.\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\CLA\\CLA_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\CLA\\CLA_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\CLA\\CLA_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\CLA\\CLA_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\CLA\\CLA_metadata.json\n",
      "\n",
      "Creating visualizations for CLA...\n",
      "Plotting with 0 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\CLA_spreads.pdf\n",
      "Completed processing for CLA\n",
      "Completed processing CLA\n",
      "\n",
      "Processing COA...\n",
      "\n",
      "Starting processing for COA\n",
      "Generated 504 tickers for COA\n",
      "\n",
      "Processing metadata for COA...\n",
      "Processing metadata batch 1/3 for COA\n",
      "Processing metadata batch 2/3 for COA\n",
      "Processing metadata batch 3/3 for COA\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for COA...\n",
      "Processing batch 1/11 for COA\n",
      "Processing batch 2/11 for COA\n",
      "Processing batch 3/11 for COA\n",
      "Processing batch 4/11 for COA\n",
      "Processing batch 5/11 for COA\n",
      "Processing batch 6/11 for COA\n",
      "Processing batch 7/11 for COA\n",
      "Processing batch 8/11 for COA\n",
      "Processing batch 9/11 for COA\n",
      "Processing batch 10/11 for COA\n",
      "Processing batch 11/11 for COA\n",
      "Processing combined data with 295 columns\n",
      "Processing raw data with 295 columns\n",
      "Found 150 price columns and 145 volume columns\n",
      "Processed 150 active futures\n",
      "Successfully processed 150 active futures\n",
      "Determined last trade dates for 150 contracts\n",
      "\n",
      "Fetching interest rate data...\n",
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for COA...\n",
      "Determined last trade dates for 150 contracts\n",
      "Processing 3465 dates...\n",
      "Processed 100 dates...\n",
      "Processed 200 dates...\n",
      "Processed 300 dates...\n",
      "Processed 400 dates...\n",
      "Processed 500 dates...\n",
      "Processed 600 dates...\n",
      "Processed 700 dates...\n",
      "Processed 800 dates...\n",
      "Processed 900 dates...\n",
      "Processed 1000 dates...\n",
      "Processed 1100 dates...\n",
      "Processed 1200 dates...\n",
      "Processed 1300 dates...\n",
      "Processed 1400 dates...\n",
      "Processed 1500 dates...\n",
      "Processed 1600 dates...\n",
      "Processed 1700 dates...\n",
      "Processed 1800 dates...\n",
      "Processed 1900 dates...\n",
      "Processed 2000 dates...\n",
      "Processed 2100 dates...\n",
      "Processed 2200 dates...\n",
      "Processed 2300 dates...\n",
      "Processed 2400 dates...\n",
      "Processed 2500 dates...\n",
      "Processed 2600 dates...\n",
      "Processed 2700 dates...\n",
      "Processed 2800 dates...\n",
      "Processed 2900 dates...\n",
      "Processed 3000 dates...\n",
      "Processed 3100 dates...\n",
      "Processed 3200 dates...\n",
      "Completed futures data organization. Processed 3251 dates with valid data.\n",
      "Identified 815 roll dates\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\COA\\COA_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\COA\\COA_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\COA\\COA_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\COA\\COA_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\COA\\COA_metadata.json\n",
      "\n",
      "Creating visualizations for COA...\n",
      "Identified 815 roll dates\n",
      "Plotting with 815 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\COA_spreads.pdf\n",
      "Completed processing for COA\n",
      "Completed processing COA\n",
      "\n",
      "Processing XBA...\n",
      "\n",
      "Starting processing for XBA\n",
      "Generated 504 tickers for XBA\n",
      "\n",
      "Processing metadata for XBA...\n",
      "Processing metadata batch 1/3 for XBA\n",
      "Processing metadata batch 2/3 for XBA\n",
      "Processing metadata batch 3/3 for XBA\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for XBA...\n",
      "Processing batch 1/11 for XBA\n",
      "Processing batch 2/11 for XBA\n",
      "Processing batch 3/11 for XBA\n",
      "Processing batch 4/11 for XBA\n",
      "Processing batch 5/11 for XBA\n",
      "Processing batch 6/11 for XBA\n",
      "Processing batch 7/11 for XBA\n",
      "Processing batch 8/11 for XBA\n",
      "Processing batch 9/11 for XBA\n",
      "Processing batch 10/11 for XBA\n",
      "Processing batch 11/11 for XBA\n",
      "Processing combined data with 186 columns\n",
      "Processing raw data with 186 columns\n",
      "Found 93 price columns and 93 volume columns\n",
      "Processed 93 active futures\n",
      "Successfully processed 93 active futures\n",
      "Determined last trade dates for 93 contracts\n",
      "\n",
      "Fetching interest rate data...\n",
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for XBA...\n",
      "Determined last trade dates for 93 contracts\n",
      "Processing 4082 dates...\n",
      "Processed 100 dates...\n",
      "Processed 200 dates...\n",
      "Processed 300 dates...\n",
      "Processed 400 dates...\n",
      "Processed 500 dates...\n",
      "Processed 600 dates...\n",
      "Processed 700 dates...\n",
      "Processed 800 dates...\n",
      "Processed 900 dates...\n",
      "Processed 1000 dates...\n",
      "Processed 1100 dates...\n",
      "Processed 1200 dates...\n",
      "Processed 1300 dates...\n",
      "Processed 1400 dates...\n",
      "Processed 1500 dates...\n",
      "Processed 1600 dates...\n",
      "Processed 1700 dates...\n",
      "Processed 1800 dates...\n",
      "Processed 1900 dates...\n",
      "Processed 2000 dates...\n",
      "Processed 2100 dates...\n",
      "Processed 2200 dates...\n",
      "Completed futures data organization. Processed 2262 dates with valid data.\n",
      "Identified 412 roll dates\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\XBA\\XBA_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\XBA\\XBA_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\XBA\\XBA_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\XBA\\XBA_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\XBA\\XBA_metadata.json\n",
      "\n",
      "Creating visualizations for XBA...\n",
      "Identified 412 roll dates\n",
      "Plotting with 412 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\XBA_spreads.pdf\n",
      "Completed processing for XBA\n",
      "Completed processing XBA\n",
      "\n",
      "Processing HOA...\n",
      "\n",
      "Starting processing for HOA\n",
      "Generated 504 tickers for HOA\n",
      "\n",
      "Processing metadata for HOA...\n",
      "Processing metadata batch 1/3 for HOA\n",
      "Processing metadata batch 2/3 for HOA\n",
      "Processing metadata batch 3/3 for HOA\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for HOA...\n",
      "Processing batch 1/11 for HOA\n",
      "Processing batch 2/11 for HOA\n",
      "Processing batch 3/11 for HOA\n",
      "Processing batch 4/11 for HOA\n",
      "Processing batch 5/11 for HOA\n",
      "Processing batch 6/11 for HOA\n",
      "Processing batch 7/11 for HOA\n",
      "Processing batch 8/11 for HOA\n",
      "Processing batch 9/11 for HOA\n",
      "Processing batch 10/11 for HOA\n",
      "Processing batch 11/11 for HOA\n",
      "Processing combined data with 90 columns\n",
      "Processing raw data with 90 columns\n",
      "Found 51 price columns and 39 volume columns\n",
      "Processed 51 active futures\n",
      "Successfully processed 51 active futures\n",
      "Determined last trade dates for 51 contracts\n",
      "\n",
      "Fetching interest rate data...\n",
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for HOA...\n",
      "Determined last trade dates for 51 contracts\n",
      "Processing 537 dates...\n",
      "Processed 100 dates...\n",
      "Processed 200 dates...\n",
      "Processed 300 dates...\n",
      "Processed 400 dates...\n",
      "Processed 500 dates...\n",
      "Completed futures data organization. Processed 520 dates with valid data.\n",
      "Identified 13 roll dates\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HOA\\HOA_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HOA\\HOA_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HOA\\HOA_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HOA\\HOA_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HOA\\HOA_metadata.json\n",
      "\n",
      "Creating visualizations for HOA...\n",
      "Identified 13 roll dates\n",
      "Plotting with 13 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\HOA_spreads.pdf\n",
      "Completed processing for HOA\n",
      "Completed processing HOA\n",
      "\n",
      "Processing NGA...\n",
      "\n",
      "Starting processing for NGA\n",
      "Generated 504 tickers for NGA\n",
      "\n",
      "Processing metadata for NGA...\n",
      "Processing metadata batch 1/3 for NGA\n",
      "Processing metadata batch 2/3 for NGA\n",
      "Processing metadata batch 3/3 for NGA\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for NGA...\n",
      "Processing batch 1/11 for NGA\n",
      "Processing batch 2/11 for NGA\n",
      "Processing batch 3/11 for NGA\n",
      "Processing batch 4/11 for NGA\n",
      "Processing batch 5/11 for NGA\n",
      "Processing batch 6/11 for NGA\n",
      "Processing batch 7/11 for NGA\n",
      "Processing batch 8/11 for NGA\n",
      "Processing batch 9/11 for NGA\n",
      "Processing batch 10/11 for NGA\n",
      "Processing batch 11/11 for NGA\n",
      "No data received from Bloomberg\n",
      "No price data found for NGA\n",
      "Completed processing NGA\n",
      "\n",
      "Processing FNA...\n",
      "\n",
      "Starting processing for FNA\n",
      "Generated 504 tickers for FNA\n",
      "\n",
      "Processing metadata for FNA...\n",
      "Processing metadata batch 1/3 for FNA\n",
      "Processing metadata batch 2/3 for FNA\n",
      "Processing metadata batch 3/3 for FNA\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for FNA...\n",
      "Processing batch 1/11 for FNA\n",
      "Processing batch 2/11 for FNA\n",
      "Processing batch 3/11 for FNA\n",
      "Processing batch 4/11 for FNA\n",
      "Processing batch 5/11 for FNA\n",
      "Processing batch 6/11 for FNA\n",
      "Processing batch 7/11 for FNA\n",
      "Processing batch 8/11 for FNA\n",
      "Processing batch 9/11 for FNA\n",
      "Processing batch 10/11 for FNA\n",
      "Processing batch 11/11 for FNA\n",
      "Processing combined data with 170 columns\n",
      "Processing raw data with 170 columns\n",
      "Found 85 price columns and 85 volume columns\n",
      "Processed 85 active futures\n",
      "Successfully processed 85 active futures\n",
      "Determined last trade dates for 85 contracts\n",
      "\n",
      "Fetching interest rate data...\n",
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for FNA...\n",
      "Determined last trade dates for 85 contracts\n",
      "Processing 6341 dates...\n",
      "Processed 100 dates...\n",
      "Processed 200 dates...\n",
      "Processed 300 dates...\n",
      "Processed 400 dates...\n",
      "Processed 500 dates...\n",
      "Processed 600 dates...\n",
      "Processed 700 dates...\n",
      "Processed 800 dates...\n",
      "Processed 900 dates...\n",
      "Processed 1000 dates...\n",
      "Processed 1100 dates...\n",
      "Processed 1200 dates...\n",
      "Processed 1300 dates...\n",
      "Processed 1400 dates...\n",
      "Processed 1500 dates...\n",
      "Processed 1600 dates...\n",
      "Processed 1700 dates...\n",
      "Processed 1800 dates...\n",
      "Processed 1900 dates...\n",
      "Processed 2000 dates...\n",
      "Processed 2100 dates...\n",
      "Completed futures data organization. Processed 2187 dates with valid data.\n",
      "Identified 66 roll dates\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\FNA\\FNA_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\FNA\\FNA_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\FNA\\FNA_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\FNA\\FNA_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\FNA\\FNA_metadata.json\n",
      "\n",
      "Creating visualizations for FNA...\n",
      "Identified 66 roll dates\n",
      "Plotting with 66 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\FNA_spreads.pdf\n",
      "Completed processing for FNA\n",
      "Completed processing FNA\n",
      "\n",
      "Processing HG...\n",
      "\n",
      "Starting processing for HG\n",
      "Generated 504 tickers for HG\n",
      "\n",
      "Processing metadata for HG...\n",
      "Processing metadata batch 1/3 for HG\n",
      "Processing metadata batch 2/3 for HG\n",
      "Processing metadata batch 3/3 for HG\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for HG...\n",
      "Processing batch 1/11 for HG\n",
      "Processing batch 2/11 for HG\n",
      "Processing batch 3/11 for HG\n",
      "Processing batch 4/11 for HG\n",
      "Processing batch 5/11 for HG\n",
      "Processing batch 6/11 for HG\n",
      "Processing batch 7/11 for HG\n",
      "Processing batch 8/11 for HG\n",
      "Processing batch 9/11 for HG\n",
      "Processing batch 10/11 for HG\n",
      "Processing batch 11/11 for HG\n",
      "Processing combined data with 898 columns\n",
      "Processing raw data with 898 columns\n",
      "Found 454 price columns and 444 volume columns\n",
      "Processed 454 active futures\n",
      "Successfully processed 454 active futures\n",
      "Determined last trade dates for 454 contracts\n",
      "\n",
      "Fetching interest rate data...\n",
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for HG...\n",
      "Determined last trade dates for 454 contracts\n",
      "Processing 9048 dates...\n",
      "Processed 100 dates...\n",
      "Processed 200 dates...\n",
      "Processed 300 dates...\n",
      "Processed 400 dates...\n",
      "Processed 500 dates...\n",
      "Processed 600 dates...\n",
      "Processed 700 dates...\n",
      "Processed 800 dates...\n",
      "Processed 900 dates...\n",
      "Processed 1000 dates...\n",
      "Processed 1100 dates...\n",
      "Processed 1200 dates...\n",
      "Processed 1300 dates...\n",
      "Processed 1400 dates...\n",
      "Processed 1500 dates...\n",
      "Processed 1600 dates...\n",
      "Processed 1700 dates...\n",
      "Processed 1800 dates...\n",
      "Processed 1900 dates...\n",
      "Processed 2000 dates...\n",
      "Processed 2100 dates...\n",
      "Processed 2200 dates...\n",
      "Processed 2300 dates...\n",
      "Processed 2400 dates...\n",
      "Processed 2500 dates...\n",
      "Processed 2600 dates...\n",
      "Processed 2700 dates...\n",
      "Processed 2800 dates...\n",
      "Processed 2900 dates...\n",
      "Processed 3000 dates...\n",
      "Processed 3100 dates...\n",
      "Processed 3200 dates...\n",
      "Processed 3300 dates...\n",
      "Processed 3400 dates...\n",
      "Processed 3500 dates...\n",
      "Processed 3600 dates...\n",
      "Processed 3700 dates...\n",
      "Processed 3800 dates...\n",
      "Processed 3900 dates...\n",
      "Processed 4000 dates...\n",
      "Processed 4100 dates...\n",
      "Processed 4200 dates...\n",
      "Processed 4300 dates...\n",
      "Processed 4400 dates...\n",
      "Processed 4500 dates...\n",
      "Processed 4600 dates...\n",
      "Processed 4700 dates...\n",
      "Processed 4800 dates...\n",
      "Processed 4900 dates...\n",
      "Processed 5000 dates...\n",
      "Processed 5100 dates...\n",
      "Processed 5200 dates...\n",
      "Processed 5300 dates...\n",
      "Processed 5400 dates...\n",
      "Processed 5500 dates...\n",
      "Processed 5600 dates...\n",
      "Processed 5700 dates...\n",
      "Processed 5800 dates...\n",
      "Processed 5900 dates...\n",
      "Processed 6000 dates...\n",
      "Processed 6100 dates...\n",
      "Processed 6200 dates...\n",
      "Processed 6300 dates...\n",
      "Processed 6400 dates...\n",
      "Processed 6500 dates...\n",
      "Processed 6600 dates...\n",
      "Processed 6700 dates...\n",
      "Processed 6800 dates...\n",
      "Processed 6900 dates...\n",
      "Processed 7000 dates...\n",
      "Processed 7100 dates...\n",
      "Processed 7200 dates...\n",
      "Processed 7300 dates...\n",
      "Processed 7400 dates...\n",
      "Processed 7500 dates...\n",
      "Processed 7600 dates...\n",
      "Processed 7700 dates...\n",
      "Processed 7800 dates...\n",
      "Processed 7900 dates...\n",
      "Processed 8000 dates...\n",
      "Processed 8100 dates...\n",
      "Processed 8200 dates...\n",
      "Processed 8300 dates...\n",
      "Processed 8400 dates...\n",
      "Processed 8500 dates...\n",
      "Processed 8600 dates...\n",
      "Processed 8700 dates...\n",
      "Processed 8800 dates...\n",
      "Processed 8900 dates...\n",
      "Completed futures data organization. Processed 8987 dates with valid data.\n",
      "Identified 503 roll dates\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HG\\HG_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HG\\HG_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HG\\HG_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HG\\HG_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\HG\\HG_metadata.json\n",
      "\n",
      "Creating visualizations for HG...\n",
      "Identified 503 roll dates\n",
      "Plotting with 503 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\HG_spreads.pdf\n",
      "Completed processing for HG\n",
      "Completed processing HG\n",
      "\n",
      "Processing ALE...\n",
      "\n",
      "Starting processing for ALE\n",
      "Generated 504 tickers for ALE\n",
      "\n",
      "Processing metadata for ALE...\n",
      "Processing metadata batch 1/3 for ALE\n",
      "Processing metadata batch 2/3 for ALE\n",
      "Processing metadata batch 3/3 for ALE\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for ALE...\n",
      "Processing batch 1/11 for ALE\n",
      "Processing batch 2/11 for ALE\n",
      "Processing batch 3/11 for ALE\n",
      "Processing batch 4/11 for ALE\n",
      "Processing batch 5/11 for ALE\n",
      "Processing batch 6/11 for ALE\n",
      "Processing batch 7/11 for ALE\n",
      "Processing batch 8/11 for ALE\n",
      "Processing batch 9/11 for ALE\n",
      "Processing batch 10/11 for ALE\n",
      "Processing batch 11/11 for ALE\n",
      "Processing combined data with 267 columns\n",
      "Processing raw data with 267 columns\n",
      "Found 150 price columns and 117 volume columns\n",
      "Processed 150 active futures\n",
      "Successfully processed 150 active futures\n",
      "Determined last trade dates for 150 contracts\n",
      "\n",
      "Fetching interest rate data...\n",
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for ALE...\n",
      "Determined last trade dates for 150 contracts\n",
      "Processing 2640 dates...\n",
      "Processed 100 dates...\n",
      "Processed 200 dates...\n",
      "Processed 300 dates...\n",
      "Processed 400 dates...\n",
      "Processed 500 dates...\n",
      "Processed 600 dates...\n",
      "Processed 700 dates...\n",
      "Processed 800 dates...\n",
      "Processed 900 dates...\n",
      "Processed 1000 dates...\n",
      "Processed 1100 dates...\n",
      "Processed 1200 dates...\n",
      "Processed 1300 dates...\n",
      "Processed 1400 dates...\n",
      "Processed 1500 dates...\n",
      "Processed 1600 dates...\n",
      "Processed 1700 dates...\n",
      "Processed 1800 dates...\n",
      "Processed 1900 dates...\n",
      "Processed 2000 dates...\n",
      "Processed 2100 dates...\n",
      "Processed 2200 dates...\n",
      "Processed 2300 dates...\n",
      "Processed 2400 dates...\n",
      "Completed futures data organization. Processed 2494 dates with valid data.\n",
      "Identified 878 roll dates\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\ALE\\ALE_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\ALE\\ALE_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\ALE\\ALE_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\ALE\\ALE_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\ALE\\ALE_metadata.json\n",
      "\n",
      "Creating visualizations for ALE...\n",
      "Identified 878 roll dates\n",
      "Plotting with 878 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\ALE_spreads.pdf\n",
      "Completed processing for ALE\n",
      "Completed processing ALE\n",
      "\n",
      "Processing GC...\n",
      "\n",
      "Starting processing for GC\n",
      "Generated 504 tickers for GC\n",
      "\n",
      "Processing metadata for GC...\n",
      "Processing metadata batch 1/3 for GC\n",
      "Processing metadata batch 2/3 for GC\n",
      "Processing metadata batch 3/3 for GC\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for GC...\n",
      "Processing batch 1/11 for GC\n",
      "Processing batch 2/11 for GC\n",
      "Processing batch 3/11 for GC\n",
      "Processing batch 4/11 for GC\n",
      "Processing batch 5/11 for GC\n",
      "Processing batch 6/11 for GC\n",
      "Processing batch 7/11 for GC\n",
      "Processing batch 8/11 for GC\n",
      "Processing batch 9/11 for GC\n",
      "Processing batch 10/11 for GC\n",
      "Processing batch 11/11 for GC\n",
      "Processing combined data with 924 columns\n",
      "Processing raw data with 924 columns\n",
      "Found 491 price columns and 433 volume columns\n",
      "Processed 490 active futures\n",
      "Successfully processed 490 active futures\n",
      "Determined last trade dates for 490 contracts\n",
      "\n",
      "Fetching interest rate data...\n",
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for GC...\n",
      "Determined last trade dates for 490 contracts\n",
      "Processing 10031 dates...\n",
      "Processed 100 dates...\n",
      "Processed 200 dates...\n",
      "Processed 300 dates...\n",
      "Processed 400 dates...\n",
      "Processed 500 dates...\n",
      "Processed 600 dates...\n",
      "Processed 700 dates...\n",
      "Processed 800 dates...\n",
      "Processed 900 dates...\n",
      "Processed 1000 dates...\n",
      "Processed 1100 dates...\n",
      "Processed 1200 dates...\n",
      "Processed 1300 dates...\n",
      "Processed 1400 dates...\n",
      "Processed 1500 dates...\n",
      "Processed 1600 dates...\n",
      "Processed 1700 dates...\n",
      "Processed 1800 dates...\n",
      "Processed 1900 dates...\n",
      "Processed 2000 dates...\n",
      "Processed 2100 dates...\n",
      "Processed 2200 dates...\n",
      "Processed 2300 dates...\n",
      "Processed 2400 dates...\n",
      "Processed 2500 dates...\n",
      "Processed 2600 dates...\n",
      "Processed 2700 dates...\n",
      "Processed 2800 dates...\n",
      "Processed 2900 dates...\n",
      "Processed 3000 dates...\n",
      "Processed 3100 dates...\n",
      "Processed 3200 dates...\n",
      "Processed 3300 dates...\n",
      "Processed 3400 dates...\n",
      "Processed 3500 dates...\n",
      "Processed 3600 dates...\n",
      "Processed 3700 dates...\n",
      "Processed 3800 dates...\n",
      "Processed 3900 dates...\n",
      "Processed 4000 dates...\n",
      "Processed 4100 dates...\n",
      "Processed 4200 dates...\n",
      "Processed 4300 dates...\n",
      "Processed 4400 dates...\n",
      "Processed 4500 dates...\n",
      "Processed 4600 dates...\n",
      "Processed 4700 dates...\n",
      "Processed 4800 dates...\n",
      "Processed 4900 dates...\n",
      "Processed 5000 dates...\n",
      "Processed 5100 dates...\n",
      "Processed 5200 dates...\n",
      "Processed 5300 dates...\n",
      "Processed 5400 dates...\n",
      "Processed 5500 dates...\n",
      "Processed 5600 dates...\n",
      "Processed 5700 dates...\n",
      "Processed 5800 dates...\n",
      "Processed 5900 dates...\n",
      "Processed 6000 dates...\n",
      "Processed 6100 dates...\n",
      "Processed 6200 dates...\n",
      "Processed 6300 dates...\n",
      "Processed 6400 dates...\n",
      "Processed 6500 dates...\n",
      "Processed 6600 dates...\n",
      "Processed 6700 dates...\n",
      "Processed 6800 dates...\n",
      "Processed 6900 dates...\n",
      "Processed 7000 dates...\n",
      "Processed 7100 dates...\n",
      "Processed 7200 dates...\n",
      "Processed 7300 dates...\n",
      "Processed 7400 dates...\n",
      "Processed 7500 dates...\n",
      "Processed 7600 dates...\n",
      "Processed 7700 dates...\n",
      "Processed 7800 dates...\n",
      "Processed 7900 dates...\n",
      "Processed 8000 dates...\n",
      "Processed 8100 dates...\n",
      "Processed 8200 dates...\n",
      "Processed 8300 dates...\n",
      "Processed 8400 dates...\n",
      "Processed 8500 dates...\n",
      "Processed 8600 dates...\n",
      "Processed 8700 dates...\n",
      "Processed 8800 dates...\n",
      "Processed 8900 dates...\n",
      "Processed 9000 dates...\n",
      "Processed 9100 dates...\n",
      "Processed 9200 dates...\n",
      "Processed 9300 dates...\n",
      "Processed 9400 dates...\n",
      "Processed 9500 dates...\n",
      "Processed 9600 dates...\n",
      "Processed 9700 dates...\n",
      "Processed 9800 dates...\n",
      "Processed 9900 dates...\n",
      "Completed futures data organization. Processed 9993 dates with valid data.\n",
      "Identified 1245 roll dates\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\GC\\GC_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\GC\\GC_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\GC\\GC_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\GC\\GC_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\GC\\GC_metadata.json\n",
      "\n",
      "Creating visualizations for GC...\n",
      "Identified 1245 roll dates\n",
      "Plotting with 1245 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\GC_spreads.pdf\n",
      "Completed processing for GC\n",
      "Completed processing GC\n",
      "\n",
      "Processing SI...\n",
      "\n",
      "Starting processing for SI\n",
      "Generated 504 tickers for SI\n",
      "\n",
      "Processing metadata for SI...\n",
      "Processing metadata batch 1/3 for SI\n",
      "Processing metadata batch 2/3 for SI\n",
      "Processing metadata batch 3/3 for SI\n",
      "Collected metadata for 504 contracts\n",
      "\n",
      "Processing price and volume data for SI...\n",
      "Processing batch 1/11 for SI\n",
      "Processing batch 2/11 for SI\n",
      "Processing batch 3/11 for SI\n",
      "Processing batch 4/11 for SI\n",
      "Processing batch 5/11 for SI\n",
      "Processing batch 6/11 for SI\n",
      "Processing batch 7/11 for SI\n",
      "Processing batch 8/11 for SI\n",
      "Processing batch 9/11 for SI\n",
      "Processing batch 10/11 for SI\n",
      "Processing batch 11/11 for SI\n",
      "Processing combined data with 952 columns\n",
      "Processing raw data with 952 columns\n",
      "Found 490 price columns and 462 volume columns\n",
      "Processed 490 active futures\n",
      "Successfully processed 490 active futures\n",
      "Determined last trade dates for 490 contracts\n",
      "\n",
      "Fetching interest rate data...\n",
      "Successfully fetched interest rate data\n",
      "\n",
      "Organizing futures data for SI...\n",
      "Determined last trade dates for 490 contracts\n",
      "Processing 10029 dates...\n",
      "Processed 100 dates...\n",
      "Processed 200 dates...\n",
      "Processed 300 dates...\n",
      "Processed 400 dates...\n",
      "Processed 500 dates...\n",
      "Processed 600 dates...\n",
      "Processed 700 dates...\n",
      "Processed 800 dates...\n",
      "Processed 900 dates...\n",
      "Processed 1000 dates...\n",
      "Processed 1100 dates...\n",
      "Processed 1200 dates...\n",
      "Processed 1300 dates...\n",
      "Processed 1400 dates...\n",
      "Processed 1500 dates...\n",
      "Processed 1600 dates...\n",
      "Processed 1700 dates...\n",
      "Processed 1800 dates...\n",
      "Processed 1900 dates...\n",
      "Processed 2000 dates...\n",
      "Processed 2100 dates...\n",
      "Processed 2200 dates...\n",
      "Processed 2300 dates...\n",
      "Processed 2400 dates...\n",
      "Processed 2500 dates...\n",
      "Processed 2600 dates...\n",
      "Processed 2700 dates...\n",
      "Processed 2800 dates...\n",
      "Processed 2900 dates...\n",
      "Processed 3000 dates...\n",
      "Processed 3100 dates...\n",
      "Processed 3200 dates...\n",
      "Processed 3300 dates...\n",
      "Processed 3400 dates...\n",
      "Processed 3500 dates...\n",
      "Processed 3600 dates...\n",
      "Processed 3700 dates...\n",
      "Processed 3800 dates...\n",
      "Processed 3900 dates...\n",
      "Processed 4000 dates...\n",
      "Processed 4100 dates...\n",
      "Processed 4200 dates...\n",
      "Processed 4300 dates...\n",
      "Processed 4400 dates...\n",
      "Processed 4500 dates...\n",
      "Processed 4600 dates...\n",
      "Processed 4700 dates...\n",
      "Processed 4800 dates...\n",
      "Processed 4900 dates...\n",
      "Processed 5000 dates...\n",
      "Processed 5100 dates...\n",
      "Processed 5200 dates...\n",
      "Processed 5300 dates...\n",
      "Processed 5400 dates...\n",
      "Processed 5500 dates...\n",
      "Processed 5600 dates...\n",
      "Processed 5700 dates...\n",
      "Processed 5800 dates...\n",
      "Processed 5900 dates...\n",
      "Processed 6000 dates...\n",
      "Processed 6100 dates...\n",
      "Processed 6200 dates...\n",
      "Processed 6300 dates...\n",
      "Processed 6400 dates...\n",
      "Processed 6500 dates...\n",
      "Processed 6600 dates...\n",
      "Processed 6700 dates...\n",
      "Processed 6800 dates...\n",
      "Processed 6900 dates...\n",
      "Processed 7000 dates...\n",
      "Processed 7100 dates...\n",
      "Processed 7200 dates...\n",
      "Processed 7300 dates...\n",
      "Processed 7400 dates...\n",
      "Processed 7500 dates...\n",
      "Processed 7600 dates...\n",
      "Processed 7700 dates...\n",
      "Processed 7800 dates...\n",
      "Processed 7900 dates...\n",
      "Processed 8000 dates...\n",
      "Processed 8100 dates...\n",
      "Processed 8200 dates...\n",
      "Processed 8300 dates...\n",
      "Processed 8400 dates...\n",
      "Processed 8500 dates...\n",
      "Processed 8600 dates...\n",
      "Processed 8700 dates...\n",
      "Processed 8800 dates...\n",
      "Processed 8900 dates...\n",
      "Processed 9000 dates...\n",
      "Processed 9100 dates...\n",
      "Processed 9200 dates...\n",
      "Processed 9300 dates...\n",
      "Processed 9400 dates...\n",
      "Processed 9500 dates...\n",
      "Processed 9600 dates...\n",
      "Processed 9700 dates...\n",
      "Processed 9800 dates...\n",
      "Processed 9900 dates...\n",
      "Completed futures data organization. Processed 9940 dates with valid data.\n",
      "Identified 1885 roll dates\n",
      "Exported dollar spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\SI\\SI_dollar_spreads.json\n",
      "Exported percent spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\SI\\SI_percent_spreads.json\n",
      "Exported annual spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\SI\\SI_annual_spreads.json\n",
      "Exported adjusted spreads to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\SI\\SI_adjusted_spreads.json\n",
      "Exported metadata to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\data\\SI\\SI_metadata.json\n",
      "\n",
      "Creating visualizations for SI...\n",
      "Identified 1885 roll dates\n",
      "Plotting with 1885 roll dates\n",
      "Visualizations saved to C:\\Users\\marti\\OneDrive\\Research\\Python Code\\Git Repository\\futures-analysis\\visualizations\\SI_spreads.pdf\n",
      "Completed processing for SI\n",
      "Completed processing SI\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\marti\\\\OneDrive\\\\Research\\\\Python Code\\\\Git Repository\\\\futures-analysis\\\\data\\\\SI\\\\SI_metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 708\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex file created at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 708\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[4], line 699\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;66;03m# Create index file for Observable\u001b[39;00m\n\u001b[0;32m    698\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[1;32m--> 699\u001b[0m index_file \u001b[38;5;241m=\u001b[39m create_observable_index(base_dir, results)\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData files saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 661\u001b[0m, in \u001b[0;36mcreate_observable_index\u001b[1;34m(output_folder, results)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m commodity, files \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m files:\n\u001b[1;32m--> 661\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    662\u001b[0m             metadata \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    663\u001b[0m         index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommodities\u001b[39m\u001b[38;5;124m'\u001b[39m][commodity] \u001b[38;5;241m=\u001b[39m metadata\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\marti\\\\OneDrive\\\\Research\\\\Python Code\\\\Git Repository\\\\futures-analysis\\\\data\\\\SI\\\\SI_metadata.json'"
     ]
    }
   ],
   "source": [
    "import blpapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Parameters\n",
    "class FuturesParams:\n",
    "    MIN_VOLUME = 0\n",
    "    MIN_DAYS_TO_EXPIRY = 0\n",
    "    START_YEAR = 1985\n",
    "    END_YEAR = 2026\n",
    "    MAX_MONTHS_FORWARD = 13\n",
    "    TRADING_DAYS_PER_YEAR = 251\n",
    "    COMMODITIES = ['CLA', 'COA', 'XBA', 'HOA', 'NGA', 'FNA', 'HG', 'ALE', 'GC', 'SI']\n",
    "\n",
    "# Bloomberg Server API settings\n",
    "HOST = 'localhost'\n",
    "PORT = 8194\n",
    "\n",
    "def setup_commodity_folders(commodity):\n",
    "    \"\"\"Create and return paths for a commodity's data\"\"\"\n",
    "    base_dir = os.getcwd()  # This should be your git repository root\n",
    "    data_dir = os.path.join(base_dir, 'data', commodity)\n",
    "    viz_dir = os.path.join(base_dir, 'visualizations')\n",
    "    excel_dir = os.path.join(base_dir, 'excel')\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    for directory in [data_dir, viz_dir, excel_dir]:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Created directory: {directory}\")\n",
    "    \n",
    "    return {\n",
    "        'data': data_dir,\n",
    "        'visualizations': viz_dir,\n",
    "        'excel': excel_dir\n",
    "    }\n",
    "\n",
    "def start_bloomberg_session():\n",
    "    \"\"\"Initialize Bloomberg API session\"\"\"\n",
    "    session_options = blpapi.SessionOptions()\n",
    "    session_options.setServerHost(HOST)\n",
    "    session_options.setServerPort(PORT)\n",
    "    session = blpapi.Session(session_options)\n",
    "    if not session.start():\n",
    "        raise Exception(\"Failed to start session.\")\n",
    "    if not session.openService(\"//blp/refdata\"):\n",
    "        raise Exception(\"Failed to open service\")\n",
    "    return session\n",
    "\n",
    "def generate_futures_tickers(commodity):\n",
    "    \"\"\"Generate all possible futures tickers for a commodity.\"\"\"\n",
    "    months = 'FGHJKMNQUVXZ'  # Bloomberg month codes\n",
    "    tickers = []\n",
    "    for year in range(FuturesParams.START_YEAR, FuturesParams.END_YEAR + 1):\n",
    "        year_str = str(year)[-2:]  # Get last 2 digits\n",
    "        for month in months:\n",
    "            tickers.append(f\"{commodity}{month}{year_str} Comdty\")\n",
    "    print(f\"Generated {len(tickers)} tickers for {commodity}\")\n",
    "    return tickers\n",
    "\n",
    "def fetch_bloomberg_data_batch(session, securities_batch, fields, start_date, end_date):\n",
    "    \"\"\"Fetch historical data for a batch of securities\"\"\"\n",
    "    refDataService = session.getService(\"//blp/refdata\")\n",
    "    \n",
    "    request = refDataService.createRequest(\"HistoricalDataRequest\")\n",
    "    for security in securities_batch:\n",
    "        request.append(\"securities\", security)\n",
    "    for field in fields:\n",
    "        request.append(\"fields\", field)\n",
    "    request.set(\"startDate\", start_date.strftime(\"%Y%m%d\"))\n",
    "    request.set(\"endDate\", end_date.strftime(\"%Y%m%d\"))\n",
    "    request.set(\"periodicityAdjustment\", \"ACTUAL\")\n",
    "    request.set(\"periodicitySelection\", \"DAILY\")\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    try:\n",
    "        session.sendRequest(request)\n",
    "        \n",
    "        while True:\n",
    "            event = session.nextEvent()\n",
    "            if event.eventType() in (blpapi.Event.PARTIAL_RESPONSE, blpapi.Event.RESPONSE):\n",
    "                for msg in event:\n",
    "                    securityData = msg.getElement(\"securityData\")\n",
    "                    security_name = securityData.getElementAsString(\"security\")\n",
    "                    fieldDataArray = securityData.getElement(\"fieldData\")\n",
    "                    \n",
    "                    for i in range(fieldDataArray.numValues()):\n",
    "                        fieldData = fieldDataArray.getValueAsElement(i)\n",
    "                        date = fieldData.getElementAsDatetime(\"date\")\n",
    "                        \n",
    "                        if date not in data_dict:\n",
    "                            data_dict[date] = {}\n",
    "                        \n",
    "                        current_data = {}\n",
    "                        for field in fields:\n",
    "                            if fieldData.hasElement(field):\n",
    "                                current_data[f\"{security_name}_{field}\"] = fieldData.getElementAsFloat(field)\n",
    "                                \n",
    "                        data_dict[date].update(current_data)\n",
    "                                \n",
    "            if event.eventType() == blpapi.Event.RESPONSE:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error fetching data for batch: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if data_dict:\n",
    "        df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.sort_index(inplace=True)\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def fetch_bloomberg_metadata_batch(session, securities_batch):\n",
    "    \"\"\"Fetch metadata for a batch of securities\"\"\"\n",
    "    refDataService = session.getService(\"//blp/refdata\")\n",
    "    metadata = {}\n",
    "    \n",
    "    try:\n",
    "        request = refDataService.createRequest(\"ReferenceDataRequest\")\n",
    "        for security in securities_batch:\n",
    "            request.append(\"securities\", security)\n",
    "        request.append(\"fields\", \"name\")\n",
    "        request.append(\"fields\", \"QUOTE_UNITS\")\n",
    "        request.append(\"fields\", \"LAST_TRADEABLE_DT\")\n",
    "        \n",
    "        session.sendRequest(request)\n",
    "        \n",
    "        while True:\n",
    "            event = session.nextEvent()\n",
    "            if event.eventType() in (blpapi.Event.PARTIAL_RESPONSE, blpapi.Event.RESPONSE):\n",
    "                for msg in event:\n",
    "                    securityDataArray = msg.getElement(\"securityData\")\n",
    "                    for i in range(securityDataArray.numValues()):\n",
    "                        securityData = securityDataArray.getValueAsElement(i)\n",
    "                        ticker = securityData.getElementAsString(\"security\")\n",
    "                        fieldData = securityData.getElement(\"fieldData\")\n",
    "                        \n",
    "                        metadata[ticker] = {\n",
    "                            'name': fieldData.getElementAsString(\"name\") if fieldData.hasElement(\"name\") else ticker,\n",
    "                            'units': fieldData.getElementAsString(\"QUOTE_UNITS\") if fieldData.hasElement(\"QUOTE_UNITS\") else '',\n",
    "                            'last_trade_date': fieldData.getElementAsString(\"LAST_TRADEABLE_DT\") if fieldData.hasElement(\"LAST_TRADEABLE_DT\") else ''\n",
    "                        }\n",
    "            if event.eventType() == blpapi.Event.RESPONSE:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error fetching metadata for batch: {e}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def fetch_interest_rates(session, start_date, end_date):\n",
    "    \"\"\"Fetch daily interest rates (GB12 Govt yields)\"\"\"\n",
    "    print(\"\\nFetching interest rate data...\")\n",
    "    try:\n",
    "        rate_data = fetch_bloomberg_data_batch(\n",
    "            session, \n",
    "            [\"GB12 Govt\"], \n",
    "            [\"PX_LAST\"], \n",
    "            start_date, \n",
    "            end_date\n",
    "        )\n",
    "        \n",
    "        if not rate_data.empty:\n",
    "            # Clean up the column name\n",
    "            rate_data.columns = ['rate']\n",
    "            # Convert to percentage\n",
    "            rate_data = rate_data / 100\n",
    "            print(\"Successfully fetched interest rate data\")\n",
    "            return rate_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching interest rate data: {e}\")\n",
    "    \n",
    "    print(\"Warning: Using zero interest rates due to data fetch failure\")\n",
    "    return pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=['rate'])\n",
    "\n",
    "def process_price_volume_data(df):\n",
    "    \"\"\"Process raw data to create price and volume dataframes\"\"\"\n",
    "    try:\n",
    "        print(f\"Processing raw data with {len(df.columns)} columns\")\n",
    "        \n",
    "        price_cols = [col for col in df.columns if 'PX_LAST' in col]\n",
    "        volume_cols = [col for col in df.columns if 'PX_VOLUME' in col]\n",
    "        \n",
    "        print(f\"Found {len(price_cols)} price columns and {len(volume_cols)} volume columns\")\n",
    "        \n",
    "        # Create separate dataframes for prices and volumes\n",
    "        prices_df = df[price_cols].copy()\n",
    "        volumes_df = df[volume_cols].copy()\n",
    "        \n",
    "        # Clean up column names\n",
    "        prices_df.columns = [col.replace('_PX_LAST', '') for col in prices_df.columns]\n",
    "        volumes_df.columns = [col.replace('_PX_VOLUME', '') for col in volumes_df.columns]\n",
    "        \n",
    "        # Set price to NaN where volume is below threshold\n",
    "        for contract in prices_df.columns:\n",
    "            if contract in volumes_df.columns:\n",
    "                mask = (volumes_df[contract].isna()) | (volumes_df[contract] < FuturesParams.MIN_VOLUME)\n",
    "                prices_df.loc[mask, contract] = np.nan\n",
    "        \n",
    "        # Sort columns chronologically\n",
    "        prices_df = prices_df.reindex(sorted(prices_df.columns), axis=1)\n",
    "        volumes_df = volumes_df.reindex(sorted(volumes_df.columns), axis=1)\n",
    "        \n",
    "        # Remove columns with all NaN values\n",
    "        prices_df = prices_df.dropna(axis=1, how='all')\n",
    "        volumes_df = volumes_df.dropna(axis=1, how='all')\n",
    "        \n",
    "        print(f\"Processed {len(prices_df.columns)} active futures\")\n",
    "        return prices_df, volumes_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_price_volume_data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_last_trade_dates(prices_df, metadata_dict):\n",
    "    \"\"\"Create mapping of contracts to their last trade dates\"\"\"\n",
    "    last_trade_dates = {}\n",
    "    \n",
    "    for contract in prices_df.columns:\n",
    "        # Try to get date from metadata first\n",
    "        meta_date = None\n",
    "        if contract in metadata_dict:\n",
    "            try:\n",
    "                meta_date = pd.to_datetime(metadata_dict[contract]['last_trade_date'])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # If no metadata date, use last price date\n",
    "        if meta_date is None:\n",
    "            meta_date = prices_df[contract].last_valid_index()\n",
    "        \n",
    "        if meta_date is not None:\n",
    "            last_trade_dates[contract] = meta_date\n",
    "    \n",
    "    print(f\"Determined last trade dates for {len(last_trade_dates)} contracts\")\n",
    "    return last_trade_dates\n",
    "\n",
    "def calculate_days_to_expiry(date, last_trade_dates):\n",
    "    \"\"\"Calculate days to expiry for each contract from a given date\"\"\"\n",
    "    days_to_expiry = {}\n",
    "    for contract, last_trade_date in last_trade_dates.items():\n",
    "        if last_trade_date >= date:\n",
    "            days = (last_trade_date - date).days\n",
    "            if days >= FuturesParams.MIN_DAYS_TO_EXPIRY:\n",
    "                days_to_expiry[contract] = days\n",
    "    return days_to_expiry\n",
    "\n",
    "def identify_roll_dates(monthly_futures_df):\n",
    "    \"\"\"Identify dates when the front month future changes\"\"\"\n",
    "    roll_dates = []\n",
    "    previous_front = None\n",
    "    front_month_col = 'month_1_future'\n",
    "    \n",
    "    if front_month_col not in monthly_futures_df.columns:\n",
    "        return roll_dates\n",
    "        \n",
    "    for date in monthly_futures_df.index:\n",
    "        current_front = monthly_futures_df.loc[date, front_month_col]\n",
    "        if pd.notna(current_front) and current_front != previous_front:\n",
    "            roll_dates.append(date)\n",
    "            previous_front = current_front\n",
    "    \n",
    "    print(f\"Identified {len(roll_dates)} roll dates\")\n",
    "    return roll_dates\n",
    "\n",
    "def create_monthly_futures_data(prices_df, metadata_dict, commodity, interest_rates_df=None):\n",
    "    \"\"\"Create dataframe with futures ordered by month and calculate spreads\"\"\"\n",
    "    print(f\"\\nOrganizing futures data for {commodity}...\")\n",
    "    \n",
    "    # Get last trade dates for all contracts\n",
    "    last_trade_dates = get_last_trade_dates(prices_df, metadata_dict)\n",
    "    \n",
    "    # Initialize dataframes for results\n",
    "    monthly_futures = pd.DataFrame(index=prices_df.index)\n",
    "    spreads_dollar = pd.DataFrame(index=prices_df.index)\n",
    "    spreads_percent = pd.DataFrame(index=prices_df.index)\n",
    "    spreads_percent_annual = pd.DataFrame(index=prices_df.index)\n",
    "    spreads_percent_annual_adjusted = pd.DataFrame(index=prices_df.index)\n",
    "    days_to_expiry_df = pd.DataFrame(index=prices_df.index)\n",
    "    \n",
    "    print(f\"Processing {len(prices_df.index)} dates...\")\n",
    "    processed_dates = 0\n",
    "    \n",
    "    for date in prices_df.index:\n",
    "        # Get valid contracts for this date\n",
    "        valid_contracts = [c for c in prices_df.columns if not pd.isna(prices_df.loc[date, c])]\n",
    "        if not valid_contracts:\n",
    "            continue\n",
    "        \n",
    "        # Calculate days to expiry and filter contracts\n",
    "        days_to_expiry = calculate_days_to_expiry(date, last_trade_dates)\n",
    "        valid_contracts = [c for c in valid_contracts if c in days_to_expiry]\n",
    "        \n",
    "        if not valid_contracts:\n",
    "            continue\n",
    "        \n",
    "        # Order contracts by expiry\n",
    "        ordered_contracts = sorted(valid_contracts, \n",
    "                                 key=lambda x: days_to_expiry.get(x, float('inf')))\n",
    "        \n",
    "        # Store prices and days to expiry for each month\n",
    "        for i, contract in enumerate(ordered_contracts, 1):\n",
    "            if i > FuturesParams.MAX_MONTHS_FORWARD:\n",
    "                break\n",
    "            monthly_futures.loc[date, f\"month_{i}_future\"] = contract\n",
    "            monthly_futures.loc[date, f\"month_{i}_price\"] = prices_df.loc[date, contract]\n",
    "            days_to_expiry_df.loc[date, f\"month_{i}_days_to_expiry\"] = days_to_expiry.get(contract)\n",
    "        \n",
    "        # Calculate spreads if we have at least two months\n",
    "        if len(ordered_contracts) >= 2:\n",
    "            m1_contract = ordered_contracts[0]\n",
    "            m1_price = prices_df.loc[date, m1_contract]\n",
    "            m1_days = days_to_expiry.get(m1_contract)\n",
    "            \n",
    "            # Calculate spreads up to max months forward\n",
    "            for i in range(1, min(FuturesParams.MAX_MONTHS_FORWARD-1, len(ordered_contracts))):\n",
    "                far_contract = ordered_contracts[i]\n",
    "                far_price = prices_df.loc[date, far_contract]\n",
    "                far_days = days_to_expiry.get(far_contract)\n",
    "                \n",
    "                if m1_price != 0 and far_days and m1_days:\n",
    "                    # Dollar spread\n",
    "                    dollar_spread = m1_price - far_price\n",
    "                    spreads_dollar.loc[date, f\"spread_1_{i+1}m\"] = dollar_spread\n",
    "                    \n",
    "                    # Basic percentage spread\n",
    "                    pct_spread = dollar_spread / m1_price\n",
    "                    spreads_percent.loc[date, f\"spread_1_{i+1}m_pct\"] = pct_spread\n",
    "                    \n",
    "                    # Calculate days difference for annualization\n",
    "                    days_difference = far_days - m1_days\n",
    "                    if days_difference > 0:\n",
    "                        # Annualize the spread\n",
    "                        annual_factor = FuturesParams.TRADING_DAYS_PER_YEAR / days_difference\n",
    "                        annual_spread = pct_spread * annual_factor\n",
    "                        spreads_percent_annual.loc[date, f\"spread_1_{i+1}m_pct_annual\"] = annual_spread\n",
    "                        \n",
    "                        # Add interest rate adjustment\n",
    "                        if interest_rates_df is not None and date in interest_rates_df.index:\n",
    "                            rate = interest_rates_df.loc[date, 'rate']\n",
    "                            adjusted_spread = annual_spread + rate  # Adding interest rate\n",
    "                            spreads_percent_annual_adjusted.loc[date, f\"spread_1_{i+1}m_pct_annual_adj\"] = adjusted_spread\n",
    "        \n",
    "        processed_dates += 1\n",
    "        if processed_dates % 100 == 0:\n",
    "            print(f\"Processed {processed_dates} dates...\")\n",
    "    \n",
    "    print(f\"Completed futures data organization. Processed {processed_dates} dates with valid data.\")\n",
    "    return (monthly_futures, spreads_dollar, spreads_percent, \n",
    "            spreads_percent_annual, spreads_percent_annual_adjusted, days_to_expiry_df)\n",
    "\n",
    "def process_commodity_data(session, commodity, start_date, end_date, metadata_batch_size=200, price_batch_size=50):\n",
    "    \"\"\"Process data for a single commodity\"\"\"\n",
    "    # Generate tickers for this commodity\n",
    "    tickers = generate_futures_tickers(commodity)\n",
    "    \n",
    "    # First fetch all metadata\n",
    "    print(f\"\\nProcessing metadata for {commodity}...\")\n",
    "    metadata_batches = np.array_split(tickers, math.ceil(len(tickers)/metadata_batch_size))\n",
    "    all_metadata = {}\n",
    "    \n",
    "    for i, batch in enumerate(metadata_batches, 1):\n",
    "        print(f\"Processing metadata batch {i}/{len(metadata_batches)} for {commodity}\")\n",
    "        try:\n",
    "            batch_metadata = fetch_bloomberg_metadata_batch(session, batch)\n",
    "            all_metadata.update(batch_metadata)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing metadata batch {i} for {commodity}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Collected metadata for {len(all_metadata)} contracts\")\n",
    "    \n",
    "    # Then fetch price and volume data\n",
    "    print(f\"\\nProcessing price and volume data for {commodity}...\")\n",
    "    price_batches = np.array_split(tickers, math.ceil(len(tickers)/price_batch_size))\n",
    "    all_data = pd.DataFrame()\n",
    "    \n",
    "    for i, batch in enumerate(price_batches, 1):\n",
    "        print(f\"Processing batch {i}/{len(price_batches)} for {commodity}\")\n",
    "        try:\n",
    "            batch_data = fetch_bloomberg_data_batch(session, batch, [\"PX_LAST\", \"PX_VOLUME\"], \n",
    "                                                  start_date, end_date)\n",
    "            if not batch_data.empty:\n",
    "                if all_data.empty:\n",
    "                    all_data = batch_data\n",
    "                else:\n",
    "                    new_cols = [col for col in batch_data.columns if col not in all_data.columns]\n",
    "                    if new_cols:\n",
    "                        batch_data = batch_data[new_cols]\n",
    "                        all_data = pd.concat([all_data, batch_data], axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing batch {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Process price and volume data\n",
    "    if not all_data.empty:\n",
    "        print(f\"Processing combined data with {len(all_data.columns)} columns\")\n",
    "        try:\n",
    "            prices_df, volumes_df = process_price_volume_data(all_data)\n",
    "            \n",
    "            if not prices_df.empty:\n",
    "                print(f\"Successfully processed {len(prices_df.columns)} active futures\")\n",
    "                return prices_df, volumes_df, all_metadata\n",
    "            else:\n",
    "                print(\"No valid price data found after processing\")\n",
    "                return pd.DataFrame(), pd.DataFrame(), all_metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data: {str(e)}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), all_metadata\n",
    "    else:\n",
    "        print(\"No data received from Bloomberg\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), all_metadata\n",
    "\n",
    "\n",
    "def create_spread_visualizations(spreads_dollar, spreads_percent, spreads_percent_annual, \n",
    "                               spreads_percent_annual_adjusted, monthly_futures, commodity, paths):\n",
    "    \"\"\"Create visualizations for all types of spreads in a single PDF\"\"\"\n",
    "    print(f\"\\nCreating visualizations for {commodity}...\")\n",
    "    \n",
    "    # Identify roll dates\n",
    "    roll_dates = identify_roll_dates(monthly_futures)\n",
    "    print(f\"Plotting with {len(roll_dates)} roll dates\")\n",
    "    \n",
    "    def create_spread_plot(df, title, ylabel, percentage=False):\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Plot spreads\n",
    "        for column in df.columns:\n",
    "            plt.plot(df.index, df[column], label=column, linewidth=1.5)\n",
    "        \n",
    "        # Add roll date lines\n",
    "        for roll_date in roll_dates:\n",
    "            plt.axvline(x=roll_date, color='gray', linewidth=0.5, alpha=0.7)\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        if percentage:\n",
    "            plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1%}'.format(y)))\n",
    "        \n",
    "        plt.grid(True, axis='y')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # Create PDF with all visualizations\n",
    "    pdf_path = os.path.join(paths['visualizations'], f'{commodity}_spreads.pdf')\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        # Dollar spreads\n",
    "        create_spread_plot(\n",
    "            spreads_dollar,\n",
    "            f'{commodity} Dollar Spreads',\n",
    "            'Spread Value'\n",
    "        )\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        # Percentage spreads\n",
    "        create_spread_plot(\n",
    "            spreads_percent,\n",
    "            f'{commodity} Percentage Spreads',\n",
    "            'Spread Percentage',\n",
    "            percentage=True\n",
    "        )\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        # Annualized percentage spreads\n",
    "        create_spread_plot(\n",
    "            spreads_percent_annual,\n",
    "            f'{commodity} Annualized Percentage Spreads',\n",
    "            'Annualized Spread Percentage',\n",
    "            percentage=True\n",
    "        )\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "        # Interest rate adjusted annualized spreads\n",
    "        create_spread_plot(\n",
    "            spreads_percent_annual_adjusted,\n",
    "            f'{commodity} Interest Rate Adjusted Annualized Spreads',\n",
    "            'Adjusted Annualized Spread Percentage',\n",
    "            percentage=True\n",
    "        )\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Visualizations saved to {pdf_path}\")\n",
    "\n",
    "def export_for_observable(spreads_dollar, spreads_percent, spreads_percent_annual, \n",
    "                        spreads_percent_annual_adjusted, monthly_futures, commodity, paths):\n",
    "    \"\"\"Export data in a format suitable for Observable\"\"\"\n",
    "    \n",
    "    def format_for_export(df, prefix):\n",
    "        # Reset index to make date a column\n",
    "        df_export = df.reset_index()\n",
    "        # Rename date column\n",
    "        df_export = df_export.rename(columns={'index': 'date'})\n",
    "        # Convert date to string in ISO format\n",
    "        df_export['date'] = df_export['date'].dt.strftime('%Y-%m-%d')\n",
    "        # Create JSON-friendly column names\n",
    "        df_export.columns = [col.replace(' ', '_').lower() for col in df_export.columns]\n",
    "        return df_export\n",
    "\n",
    "    # Format each type of spread\n",
    "    dollar_export = format_for_export(spreads_dollar, 'dollar')\n",
    "    percent_export = format_for_export(spreads_percent, 'percent')\n",
    "    annual_export = format_for_export(spreads_percent_annual, 'annual')\n",
    "    adjusted_export = format_for_export(spreads_percent_annual_adjusted, 'adjusted')\n",
    "\n",
    "    # Get roll dates\n",
    "    roll_dates = identify_roll_dates(monthly_futures)\n",
    "    roll_dates_list = [d.strftime('%Y-%m-%d') for d in roll_dates]\n",
    "\n",
    "    # Save as JSON files\n",
    "    output_files = {}\n",
    "    try:\n",
    "        for name, df in [\n",
    "            ('dollar', dollar_export),\n",
    "            ('percent', percent_export),\n",
    "            ('annual', annual_export),\n",
    "            ('adjusted', adjusted_export)\n",
    "        ]:\n",
    "            filename = os.path.join(paths['data'], f'{commodity}_{name}_spreads.json')\n",
    "            df.to_json(filename, orient='records', date_format='iso')\n",
    "            output_files[name] = filename\n",
    "            print(f\"Exported {name} spreads to {filename}\")\n",
    "\n",
    "        # Save metadata with roll dates\n",
    "        metadata = {\n",
    "            'commodity': commodity,\n",
    "            'date_range': {\n",
    "                'start': str(spreads_dollar.index.min().date()),\n",
    "                'end': str(spreads_dollar.index.max().date())\n",
    "            },\n",
    "            'series_info': {\n",
    "                'dollar': list(spreads_dollar.columns),\n",
    "                'percent': list(spreads_percent.columns),\n",
    "                'annual': list(spreads_percent_annual.columns),\n",
    "                'adjusted': list(spreads_percent_annual_adjusted.columns)\n",
    "            },\n",
    "            'roll_dates': roll_dates_list\n",
    "        }\n",
    "        \n",
    "        metadata_file = os.path.join(paths['data'], f'{commodity}_metadata.json')\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        output_files['metadata'] = metadata_file\n",
    "        print(f\"Exported metadata to {metadata_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting data: {e}\")\n",
    "        \n",
    "    return output_files\n",
    "\n",
    "def process_single_commodity(commodity, start_date, end_date):\n",
    "    \"\"\"Process a single commodity - to be run in parallel\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nStarting processing for {commodity}\")\n",
    "        session = start_bloomberg_session()\n",
    "        \n",
    "        try:\n",
    "            # Setup paths for this commodity\n",
    "            paths = setup_commodity_folders(commodity)\n",
    "            \n",
    "            # Get raw data\n",
    "            prices_df, volumes_df, metadata = process_commodity_data(\n",
    "                session, commodity, start_date, end_date, \n",
    "                metadata_batch_size=200, price_batch_size=50\n",
    "            )\n",
    "            \n",
    "            if prices_df.empty:\n",
    "                print(f\"No price data found for {commodity}\")\n",
    "                return None\n",
    "            \n",
    "            # Filter metadata to only include active futures\n",
    "            active_futures = prices_df.columns\n",
    "            metadata = {k: v for k, v in metadata.items() if k in active_futures}\n",
    "            \n",
    "            # Get last trade dates\n",
    "            last_trade_dates = get_last_trade_dates(prices_df, metadata)\n",
    "            \n",
    "            # Get interest rates\n",
    "            interest_rates_df = fetch_interest_rates(session, start_date, end_date)\n",
    "            \n",
    "            # Create monthly futures data and calculate spreads\n",
    "            (monthly_futures, spreads_dollar, spreads_percent, \n",
    "             spreads_percent_annual, spreads_percent_annual_adjusted, \n",
    "             days_to_expiry) = create_monthly_futures_data(\n",
    "                prices_df, metadata, commodity, interest_rates_df)\n",
    "            \n",
    "            # Export data for Observable\n",
    "            output_files = export_for_observable(\n",
    "                spreads_dollar, spreads_percent, spreads_percent_annual,\n",
    "                spreads_percent_annual_adjusted, monthly_futures, \n",
    "                commodity, paths\n",
    "            )\n",
    "            \n",
    "            # Create visualizations\n",
    "            create_spread_visualizations(\n",
    "                spreads_dollar, spreads_percent, spreads_percent_annual,\n",
    "                spreads_percent_annual_adjusted, monthly_futures,\n",
    "                commodity, paths\n",
    "            )\n",
    "            \n",
    "            # Save Excel file\n",
    "            excel_path = os.path.join(paths['excel'], f'futures_analysis_{commodity}.xlsx')\n",
    "            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "                prices_df.to_excel(writer, sheet_name='Prices')\n",
    "                volumes_df.to_excel(writer, sheet_name='Volumes')\n",
    "                monthly_futures.to_excel(writer, sheet_name='Monthly')\n",
    "                days_to_expiry.to_excel(writer, sheet_name='DaysToExpiry')\n",
    "                spreads_dollar.to_excel(writer, sheet_name='Spreads_Dollar')\n",
    "                spreads_percent.to_excel(writer, sheet_name='Spreads_Pct')\n",
    "                spreads_percent_annual.to_excel(writer, sheet_name='Spreads_Annual')\n",
    "                spreads_percent_annual_adjusted.to_excel(writer, sheet_name='Spreads_Annual_Adj')\n",
    "                pd.DataFrame.from_dict(metadata, orient='index').to_excel(writer, sheet_name='Metadata')\n",
    "                pd.Series(last_trade_dates).to_frame('Last_Trade_Date').to_excel(writer, sheet_name='LastTradeDates')\n",
    "                interest_rates_df.to_excel(writer, sheet_name='Rates')\n",
    "            \n",
    "            print(f\"Completed processing for {commodity}\")\n",
    "            return output_files\n",
    "            \n",
    "        finally:\n",
    "            session.stop()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {commodity}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_observable_index(output_folder, results):\n",
    "    \"\"\"Create an index file for Observable with information about all commodities\"\"\"\n",
    "    index = {\n",
    "        'commodities': {},\n",
    "        'date_range': {\n",
    "            'start': str(datetime(FuturesParams.START_YEAR, 1, 1).date()),\n",
    "            'end': str(datetime.now().date())\n",
    "        },\n",
    "        'last_updated': str(datetime.now())\n",
    "    }\n",
    "    \n",
    "    for commodity, files in results.items():\n",
    "        if files:\n",
    "            with open(files['metadata'], 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            index['commodities'][commodity] = metadata\n",
    "    \n",
    "    index_file = os.path.join(output_folder, 'index.json')\n",
    "    with open(index_file, 'w') as f:\n",
    "        json.dump(index, f, indent=2)\n",
    "    \n",
    "    print(f\"Created index file at {index_file}\")\n",
    "    return index_file\n",
    "\n",
    "def main():\n",
    "    print(f\"\"\"Initializing futures analysis with parameters:\n",
    "    Minimum Volume: {FuturesParams.MIN_VOLUME}\n",
    "    Minimum Days to Expiry: {FuturesParams.MIN_DAYS_TO_EXPIRY}\n",
    "    Date Range: {FuturesParams.START_YEAR}-{FuturesParams.END_YEAR}\n",
    "    Maximum Months Forward: {FuturesParams.MAX_MONTHS_FORWARD}\n",
    "    Trading Days Per Year: {FuturesParams.TRADING_DAYS_PER_YEAR}\n",
    "    Processing Commodities: {', '.join(FuturesParams.COMMODITIES)}\n",
    "    \"\"\")\n",
    "    \n",
    "    start_date = datetime(FuturesParams.START_YEAR, 1, 1)\n",
    "    end_date = datetime.now()\n",
    "    \n",
    "    # Process commodities sequentially\n",
    "    results = {}\n",
    "    \n",
    "    for commodity in FuturesParams.COMMODITIES:\n",
    "        try:\n",
    "            print(f\"\\nProcessing {commodity}...\")\n",
    "            result = process_single_commodity(commodity, start_date, end_date)\n",
    "            results[commodity] = result\n",
    "            print(f\"Completed processing {commodity}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {commodity}: {str(e)}\")\n",
    "    \n",
    "    # Create index file for Observable\n",
    "    base_dir = os.getcwd()\n",
    "    index_file = create_observable_index(base_dir, results)\n",
    "    \n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Data files saved in: {os.path.join(base_dir, 'data')}\")\n",
    "    print(f\"Visualizations saved in: {os.path.join(base_dir, 'visualizations')}\")\n",
    "    print(f\"Excel files saved in: {os.path.join(base_dir, 'excel')}\")\n",
    "    print(f\"Index file created at: {index_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c4066-d616-4861-9d4c-1980bb563e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
